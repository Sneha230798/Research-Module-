{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_staggered_law_ar1_data_power(N, T, rho, num_individuals, mean=0, std_dev=1):\n",
    "    # Generate random white noise for each individual\n",
    "    white_noise = np.random.normal(mean, std_dev, size=(N, num_individuals, T))\n",
    "\n",
    "    # Initialize the array to store the data\n",
    "    data = np.zeros((N, num_individuals, T))\n",
    "\n",
    "    # Generate the AR(1) process data for each individual\n",
    "    for i in range(N):\n",
    "        for j in range(num_individuals):\n",
    "            for t in range(T):\n",
    "                if t == 0:\n",
    "                    data[i, j, t] = white_noise[i, j, t]\n",
    "                else:\n",
    "                    data[i, j, t] = rho * data[i, j, t - 1] + white_noise[i, j, t]\n",
    "\n",
    "    # Reshape the data array for easier DataFrame creation\n",
    "    reshaped_data = data.reshape((N * num_individuals, T))\n",
    "\n",
    "    # Create a DataFrame with column names as time periods\n",
    "    df = pd.DataFrame(reshaped_data, columns=[f'{t}' for t in range(T)])\n",
    "\n",
    "    # Add a new 'state' column with repeated state values\n",
    "    df['state'] = np.repeat(np.arange(1, N + 1), num_individuals)\n",
    "\n",
    "    # Add a new 'individual' column with repeated individual values\n",
    "    df['individual'] = np.tile(np.arange(1, num_individuals + 1), N)\n",
    "\n",
    "    melted_df = pd.melt(df, id_vars=['state', 'individual'], var_name='time', value_name='value')\n",
    "\n",
    "    # Convert the 'time' column to int\n",
    "    melted_df['time'] = melted_df['time'].astype(int)\n",
    "\n",
    "    data = melted_df.copy()\n",
    "\n",
    "    data['time'] = data['time'].astype(int)\n",
    "    # Create state dummy variables\n",
    "    state_dummies = pd.get_dummies(data['state'], prefix='state', drop_first = True)\n",
    "\n",
    "    # Convert state dummy variables to int\n",
    "    state_dummies = state_dummies.astype(int)\n",
    "\n",
    "    # Create time dummy variables\n",
    "    time_dummies = pd.get_dummies(data['time'].astype(int), prefix='time', drop_first = True)\n",
    "\n",
    "    # Convert time dummy variables to int\n",
    "    time_dummies = time_dummies.astype(int)\n",
    "\n",
    "    data = pd.concat([data, state_dummies, time_dummies], axis=1)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sneha\\.ipynb_checkpoints\\.conda\\lib\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\sneha\\.ipynb_checkpoints\\.conda\\lib\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m state_to_treatment_year \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(treatment_states, treatment_years))\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Add a treatment column to the DataFrame\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTREATMENT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtreatment_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate_to_treatment_year\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutcome\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1.02\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTREATMENT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     40\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutcome\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutcome\u001b[39m\u001b[38;5;124m'\u001b[39m])    \n",
      "File \u001b[1;32mc:\\Users\\sneha\\.ipynb_checkpoints\\.conda\\lib\\site-packages\\pandas\\core\\frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9557\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9559\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m   9560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9561\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9566\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   9567\u001b[0m )\n\u001b[1;32m-> 9568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sneha\\.ipynb_checkpoints\\.conda\\lib\\site-packages\\pandas\\core\\apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[1;32m--> 764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sneha\\.ipynb_checkpoints\\.conda\\lib\\site-packages\\pandas\\core\\apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 891\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mc:\\Users\\sneha\\.ipynb_checkpoints\\.conda\\lib\\site-packages\\pandas\\core\\apply.py:905\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    902\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 905\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m    906\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m    907\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(v)\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    909\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    910\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sneha\\.ipynb_checkpoints\\.conda\\lib\\site-packages\\pandas\\core\\apply.py:1018\u001b[0m, in \u001b[0;36mFrameColumnApply.series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mseries_generator\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1018\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n\u001b[0;32m   1019\u001b[0m     values \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(values)\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sneha\\.ipynb_checkpoints\\.conda\\lib\\site-packages\\pandas\\_libs\\properties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\sneha\\.ipynb_checkpoints\\.conda\\lib\\site-packages\\pandas\\core\\apply.py:725\u001b[0m, in \u001b[0;36mFrameApply.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sneha\\.ipynb_checkpoints\\.conda\\lib\\site-packages\\pandas\\core\\frame.py:11739\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  11666\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  11667\u001b[0m \u001b[38;5;124;03mReturn a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[0;32m  11668\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11736\u001b[0m \u001b[38;5;124;03m       ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[0;32m  11737\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  11738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[1;32m> 11739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sneha\\.ipynb_checkpoints\\.conda\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1770\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1768\u001b[0m             arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1770\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sneha\\.ipynb_checkpoints\\.conda\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1836\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1834\u001b[0m         arr \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mget_values(dtype)\n\u001b[0;32m   1835\u001b[0m     result[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m arr\n\u001b[1;32m-> 1836\u001b[0m     itemmask[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m itemmask\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m   1839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome items were not contained in blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "beta1_estimates = []\n",
    "reject_count = 0  # Counter for the number of rejections\n",
    "alpha = 0.05  # Significance level\n",
    "rho = 0.8\n",
    "\n",
    "bias_values = []\n",
    "\n",
    "N = 50\n",
    "T = 20\n",
    "true_beta1_value = 0.02 \n",
    "squared_error_values = []\n",
    "standard_error_values =[]\n",
    "num_simulations = 50\n",
    "\n",
    "\n",
    "for _ in range(num_simulations):\n",
    "    data = generate_staggered_law_ar1_data_power(N, T, rho, num_individuals= 500)\n",
    "    \n",
    "    states = data['state'].unique()\n",
    "    \n",
    "\n",
    "    # Randomly select half of the states to be in the treatment group\n",
    "    \n",
    "    treatment_states = np.random.choice(states, size=len(states)//2, replace=False)\n",
    "\n",
    "    # Assign treatment year to each treatment state, staggered between 1985 and 1995\n",
    "    treatment_years = np.random.choice(range(1985, 1995), size=len(treatment_states), replace=True)\n",
    "    state_to_treatment_year = dict(zip(treatment_states, treatment_years))\n",
    "\n",
    "    # Add a treatment column to the DataFrame\n",
    "    data['TREATMENT'] = data.apply(lambda x: 1 if x['state'] in treatment_states and x['time'] >= state_to_treatment_year[x['state']] else 0, axis=1)\n",
    "    \n",
    "    data['outcome'] = data.apply(lambda x: x['value']*(1.02) if x['TREATMENT'] == 1 else x['value'], axis=1)\n",
    "       \n",
    "\n",
    "    cps_agg = data.groupby(['state', 'time'])[['outcome', 'TREATMENT']].mean().reset_index()\n",
    "\n",
    "    # One-hot encode STATEFIP and YEAR\n",
    "    dummy_df_state = pd.get_dummies(cps_agg['state'], prefix='state', drop_first=True)\n",
    "    dummy_df_year = pd.get_dummies(cps_agg['time'], prefix='time', drop_first=True)\n",
    "\n",
    "    # Concatenate the dummy variables with the original DataFrame\n",
    "    cps_agg = pd.concat([cps_agg, dummy_df_state, dummy_df_year], axis=1)\n",
    "\n",
    "    # Convert True and False to 1 and 0 in the specified columns\n",
    "    boolean_columns = ['state_2', 'state_3', 'state_4', 'state_5',\n",
    "       'state_6', 'state_7', 'state_8', 'state_9', 'state_10', 'state_11',\n",
    "       'state_12', 'state_13', 'state_14', 'state_15', 'state_16', 'state_17',\n",
    "       'state_18', 'state_19', 'state_20', 'state_21', 'state_22', 'state_23',\n",
    "       'state_24', 'state_25', 'state_26', 'state_27', 'state_28', 'state_29',\n",
    "       'state_30', 'state_31', 'state_32', 'state_33', 'state_34', 'state_35',\n",
    "       'state_36', 'state_37', 'state_38', 'state_39', 'state_40', 'state_41',\n",
    "       'state_42', 'state_43', 'state_44', 'state_45', 'state_46', 'state_47',\n",
    "       'state_48', 'state_49', 'state_50', 'time_1', 'time_2', 'time_3',\n",
    "       'time_4', 'time_5', 'time_6', 'time_7', 'time_8', 'time_9', 'time_10',\n",
    "       'time_11', 'time_12', 'time_13', 'time_14', 'time_15', 'time_16',\n",
    "       'time_17', 'time_18', 'time_19']\n",
    "\n",
    "    cps_agg[boolean_columns] = cps_agg[boolean_columns].astype(int)\n",
    "\n",
    "    data = cps_agg.copy()\n",
    "\n",
    "    X = data[['TREATMENT',\n",
    "       'state_2', 'state_3', 'state_4', 'state_5',\n",
    "       'state_6', 'state_7', 'state_8', 'state_9', 'state_10', 'state_11',\n",
    "       'state_12', 'state_13', 'state_14', 'state_15', 'state_16', 'state_17',\n",
    "       'state_18', 'state_19', 'state_20', 'state_21', 'state_22', 'state_23',\n",
    "       'state_24', 'state_25', 'state_26', 'state_27', 'state_28', 'state_29',\n",
    "       'state_30', 'state_31', 'state_32', 'state_33', 'state_34', 'state_35',\n",
    "       'state_36', 'state_37', 'state_38', 'state_39', 'state_40', 'state_41',\n",
    "       'state_42', 'state_43', 'state_44', 'state_45', 'state_46', 'state_47',\n",
    "       'state_48', 'state_49', 'state_50', 'time_1', 'time_2', 'time_3',\n",
    "       'time_4', 'time_5', 'time_6', 'time_7', 'time_8', 'time_9', 'time_10',\n",
    "       'time_11', 'time_12', 'time_13', 'time_14', 'time_15', 'time_16',\n",
    "       'time_17', 'time_18', 'time_19' ]] # plus any other control variables\n",
    "    X = sm.add_constant(X)\n",
    "    Y = data['outcome'] # Replace 'outcome' with your dependent variable\n",
    "    model = sm.OLS(Y, X).fit(cov_type='cluster', cov_kwds={'groups': data['state'].astype(str)})\n",
    "\n",
    "    bias = model.params['TREATMENT'] - true_beta1_value\n",
    "    \n",
    "    bias_values.append(bias)\n",
    "\n",
    "    squared_error = (model.params['TREATMENT'] - true_beta1_value) ** 2\n",
    "\n",
    "    \n",
    "    standard_error = model.bse['TREATMENT']\n",
    "    standard_error_values.append(standard_error)\n",
    "    beta1_estimates.append(model.params['TREATMENT'])\n",
    "    \n",
    "    \n",
    "    # Check if null hypothesis for beta1 is rejected\n",
    "    if model.pvalues['TREATMENT'] < alpha:\n",
    "        reject_count += 1\n",
    "\n",
    "\n",
    "type1_error = reject_count / num_simulations\n",
    "\n",
    "average_bias = np.mean(bias_values)\n",
    "average_mse = np.mean(squared_error)   \n",
    "average_rmse = np.sqrt(average_mse)  \n",
    "average_standard_error = np.mean(standard_error_values)   \n",
    "\n",
    "std_error_beta_distribution = np.std(beta1_estimates)\n",
    "\n",
    "\n",
    "average_bias = np.mean(bias_values)\n",
    "average_mse = np.mean(squared_error)\n",
    "\n",
    "\n",
    "# Print the number of rejections\n",
    "print(f\"Number of times null hypothesis is rejected : {reject_count} out of {num_simulations} simulations\")\n",
    "print(f\"Power of the test: {type1_error * 100} %\")\n",
    "print(f\"Bias for Coefficient of Treatment (True Value = {true_beta1_value}): {average_bias}\")\n",
    "print(f\"MSE for Coefficient of Treatment (True Value = {true_beta1_value}): {average_mse}\")\n",
    "\n",
    "sns.histplot(beta1_estimates, kde=True)\n",
    "plt.xlabel('Beta1 Estimates')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Beta1 Estimates')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
